version: '3.3'

services:
  app:
    # Basic image that I created for the article.
    # But you are free to swap this for any image that you'd like.
    # Just remember to change the ports configuration.
    image: snieking/haproxy-example:1.0
    ports:
      # Image that I created runs on port 12000.
      # We do not need to expose it as all the requests will go through
      # the load balancer instead.
      - 12000
    environment:
      # Port used by the HAProxy to reach the service.
      - SERVICE_PORTS=12000
    # Deploy configuration, 3 nodes will be attempted to be deployed.
    # We can scale this up and down after as we would like.
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
        max_attempts: 3
        window: 120s
    networks:
      - web

  proxy:
    image: dockercloud/haproxy
    # Won't start until at least one of our app services is up and running.
    depends_on:
      - app
    environment:
      # The type of load balancing strategy that will be used.
      # - leastconn sends request to the service with the least active requests.
      # - roundrobin rotates the requests around the services.
      - BALANCE=leastconn
      # Used to identify services.
      - ADDITIONAL_SERVICES=project_dir:app
    volumes:
      # Since our app services are running on the same port,
      # the HAProxy will use the docker.sock to find the
      # services that it should load balance.
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      # The internal used by the HAProxy is 80,
      # but we can expose any port that we would like externally.
      # For example, if you are running something else on 80,
      # you probably don't want to expose the HAProxy on 80 as well.
      - 12001:80
    networks:
      - web
    deploy:
      # The HAProxy is assigned as the manager.
      placement:
        constraints: [node.role == manager]

# Overlay network that the services and the HAProxy exists in.
networks:
  web:
    driver: overlay